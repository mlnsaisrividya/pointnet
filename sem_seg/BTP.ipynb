{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gross-selection",
   "metadata": {},
   "source": [
    "# latest code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import h5py\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)\n",
    "\n",
    "# SOME_FILES = [line.rstrip() for line in open('indoor3d_sem_seg_hdf5_data/someFiles.txt')]#provider.getDataFiles('indoor3d_sem_seg_hdf5_data/all_files.txt')\n",
    "# # Load ALL data\n",
    "# data_batch_list = []\n",
    "# label_batch_list = []\n",
    "# for h5_filename in SOME_FILES:\n",
    "#     data_batch, label_batch = load_h5(h5_filename)\n",
    "#     data_batch_list.append(data_batch)\n",
    "#     label_batch_list.append(label_batch)\n",
    "# data_batches = np.concatenate(data_batch_list, 0)\n",
    "# label_batches = np.concatenate(label_batch_list, 0)\n",
    "\n",
    "data_batch, label_batch = load_h5('indoor3d_sem_seg_hdf5_data/ply_data_all_5.h5')\n",
    "data_batches = data_batch[0:100,:,:]\n",
    "label_batches = label_batch[0:100,:]\n",
    "print(data_batches.shape)\n",
    "print(label_batches.shape)\n",
    "#cat_labels = tf.keras.utils.to_categorical(label_batches, num_classes = 13, dtype = \"uint8\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( data_batches, label_batches , test_size=0.2, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "input_image = keras.Input(shape = (4096,9,1))                                                            #B*N*9*1\n",
    "num_point = 4096\n",
    "x = Conv2D(64, [1,9], padding='VALID', strides = [1,1], activation = 'relu', name ='conv1')(input_image) #B*N*1*64\n",
    "print(x.shape)\n",
    "x = Conv2D(64, [1,1], padding='VALID', strides = [1,1], activation = 'relu', name ='conv2')(x)           #B*N*1*64\n",
    "x = Conv2D(64, [1,1], padding='VALID', strides = [1,1], activation = 'relu', name ='conv3')(x)           #B*N*1*64\n",
    "x = Conv2D(128, [1,1], padding='VALID', strides = [1,1],activation = 'relu', name ='conv4')(x)           #B*N*1*128\n",
    "x= Conv2D(1024, [1,1], padding='VALID', strides = [1,1], activation = 'relu',name ='conv5')(x)           #B*N*1*1024\n",
    "                                                                    #Now x is partial feature vector of each point\n",
    "    # max pooling as a symmetric function is a key point to solve the cloud of disorder.\n",
    "print(x.shape)\n",
    "pc_feat1 = MaxPooling2D([num_point,1],strides=(2, 2), padding='VALID', name ='maxpool1')(x)   #maxpooling for each channel \n",
    "print(pc_feat1.shape)\n",
    "pc_feat1 = Flatten()(pc_feat1)\n",
    "print(pc_feat1.shape)\n",
    "#model.add(Flatten())\n",
    "pc_feat1 = Dense(256, activation = 'relu', name='fc1')(pc_feat1)        #global feature vectors through fully connected layers                                                      \n",
    "pc_feat1 = Dense(128 , activation = 'relu', name='fc2')(pc_feat1)\n",
    "print(pc_feat1.shape)\n",
    "# CONCAT\n",
    "a = tf.keras.layers.Reshape((1, 1, -1))(pc_feat1)                   #B*1*1*128\n",
    "print(a.shape)\n",
    "\n",
    "pc_feat1_expand = tf.tile(a, [1, num_point, 1, 1])                  #B*N*1*128\n",
    "print(pc_feat1_expand.shape)\n",
    "points_feat1_concat = tf.concat(axis=3, values=[x, pc_feat1_expand])#B*N*1*1152    (1024+128)axis = 3 implies along 4th column\n",
    "print(points_feat1_concat.shape)\n",
    "# CONV\n",
    "\n",
    "y = Conv2D(512, [1, 1], padding='VALID', strides=[1, 1], activation = 'relu', name='conv6')(points_feat1_concat)\n",
    "y = Conv2D(256, [1, 1], padding='VALID', strides=[1, 1], activation = 'relu', name='conv7')(y)\n",
    "# tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "\n",
    "y = Dropout(rate=0.3)(y)                                                #rate = Fraction of the input units to drop.\n",
    "y = Conv2D(13, [1, 1], padding='VALID', strides=[1, 1], activation = 'softmax', name='conv8')(y)#B*N*1*13\n",
    "net = tf.squeeze(y, [2])                                                #B*N*13\n",
    "\n",
    "# return net\n",
    "model = keras.Model(inputs=input_image, outputs=net)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=24, epochs=10, verbose=1)     #verbose=0 will show you nothing (silent)\n",
    "model.evaluate(X_test, Y_test, verbose=2)    #verbose=1 will show you an animated progress bar & verbose=2 will just mention the number of epoch\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "                                                    #pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "                                                    #pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-chamber",
   "metadata": {},
   "source": [
    "# MY CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import h5py\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)\n",
    "\n",
    "ALL_FILES = [line.rstrip() for line in open('indoor3d_sem_seg_hdf5_data/all_files.txt')]#provider.getDataFiles('indoor3d_sem_seg_hdf5_data/all_files.txt')\n",
    "# Load ALL data\n",
    "data_batch_list = []\n",
    "label_batch_list = []\n",
    "for h5_filename in ALL_FILES:\n",
    "    data_batch, label_batch = load_h5(h5_filename)\n",
    "    data_batch_list.append(data_batch)\n",
    "    label_batch_list.append(label_batch)\n",
    "data_batches = np.concatenate(data_batch_list, 0)\n",
    "label_batches = np.concatenate(label_batch_list, 0)\n",
    "print(data_batches.shape)\n",
    "print(label_batches.shape)\n",
    "\n",
    "cat_labels = tf.keras.utils.to_categorical(label_batches, num_classes = 13, dtype = \"uint8\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( data_batches, cat_labels , test_size=0.2, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "input_image = keras.Input(shape = (4096,9,1))\n",
    "num_point = 4096\n",
    "x = Conv2D(64, [1,9], padding='VALID', strides = [1,1], name ='conv1')(input_image)\n",
    "print(x.shape)\n",
    "x = Conv2D(64, [1,1], padding='VALID', strides = [1,1], name ='conv2')(x)\n",
    "x = Conv2D(64, [1,1], padding='VALID', strides = [1,1], name ='conv3')(x)\n",
    "x = Conv2D(128, [1,1], padding='VALID', strides = [1,1], name ='conv4')(x)\n",
    "x= Conv2D(1024, [1,1], padding='VALID', strides = [1,1], name ='conv5')(x)\n",
    "    # MAX\n",
    "print(x.shape)\n",
    "pc_feat1 = MaxPooling2D([num_point,1], padding='VALID', name ='maxpool1')(x)\n",
    "print(pc_feat1.shape)\n",
    "pc_feat1 = Flatten()(pc_feat1)\n",
    "print(pc_feat1.shape)\n",
    "#model.add(Flatten())\n",
    "pc_feat1 = Dense(256, name='fc1')(pc_feat1)\n",
    "pc_feat1 = Dense(128 , name='fc2')(pc_feat1)\n",
    "print(pc_feat1.shape)\n",
    "# CONCAT\n",
    "a = tf.keras.layers.Reshape((1, 1, -1))(pc_feat1)\n",
    "print(a.shape)\n",
    "\n",
    "pc_feat1_expand = tf.tile(a, [1, num_point, 1, 1])\n",
    "print(pc_feat1_expand.shape)\n",
    "points_feat1_concat = tf.concat(axis=3, values=[x, pc_feat1_expand])\n",
    "print(points_feat1_concat.shape)\n",
    "# CONV\n",
    "\n",
    "y = Conv2D(512, [1, 1], padding='VALID', strides=[1, 1], name='conv6')(points_feat1_concat)\n",
    "y = Conv2D(256, [1, 1], padding='VALID', strides=[1, 1], name='conv7')(y)\n",
    "# tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "\n",
    "y = Dropout(rate=0.3)(y)\n",
    "y = Conv2D(13, [1, 1], padding='VALID', strides=[1, 1], name='conv8')(y)\n",
    "net = tf.squeeze(y, [2])\n",
    "\n",
    "# return net\n",
    "model = keras.Model(inputs=input_image, outputs=net)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=24, epochs=3, verbose=1)\n",
    "model.evaluate(X_test, Y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-child",
   "metadata": {},
   "source": [
    "\n",
    "# Available Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-hurricane",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-lesbian",
   "metadata": {},
   "source": [
    "#### download data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HDF5 for indoor 3d semantic segmentation (around 1.6GB)\n",
    "wget https://shapenet.cs.stanford.edu/media/indoor3d_sem_seg_hdf5_data.zip\n",
    "unzip indoor3d_sem_seg_hdf5_data.zip\n",
    "rm indoor3d_sem_seg_hdf5_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-stand",
   "metadata": {},
   "source": [
    "#### batch inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(BASE_DIR)\n",
    "from model import *\n",
    "import indoor3d_util\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='Batch Size during training [default: 1]')\n",
    "parser.add_argument('--num_point', type=int, default=4096, help='Point number [default: 4096]')\n",
    "parser.add_argument('--model_path', required=True, help='model checkpoint file path')\n",
    "parser.add_argument('--dump_dir', required=True, help='dump folder path')\n",
    "parser.add_argument('--output_filelist', required=True, help='TXT filename, filelist, each line is an output for a room')\n",
    "parser.add_argument('--room_data_filelist', required=True, help='TXT filename, filelist, each line is a test room data label file.')\n",
    "parser.add_argument('--no_clutter', action='store_true', help='If true, donot count the clutter class')\n",
    "parser.add_argument('--visu', action='store_true', help='Whether to output OBJ file for prediction visualization.')\n",
    "FLAGS = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "MODEL_PATH = FLAGS.model_path\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "DUMP_DIR = FLAGS.dump_dir\n",
    "if not os.path.exists(DUMP_DIR): os.mkdir(DUMP_DIR)\n",
    "LOG_FOUT = open(os.path.join(DUMP_DIR, 'log_evaluate.txt'), 'w')\n",
    "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "ROOM_PATH_LIST = [os.path.join(ROOT_DIR,line.rstrip()) for line in open(FLAGS.room_data_filelist)]\n",
    "\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def evaluate():\n",
    "    is_training = False\n",
    "     \n",
    "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "        pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "        # simple model\n",
    "        pred = get_model(pointclouds_pl, is_training_pl)\n",
    "        loss = get_loss(pred, labels_pl)\n",
    "        pred_softmax = tf.nn.softmax(pred)\n",
    " \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    # Create a session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "    log_string(\"Model restored.\")\n",
    "\n",
    "    ops = {'pointclouds_pl': pointclouds_pl,\n",
    "           'labels_pl': labels_pl,\n",
    "           'is_training_pl': is_training_pl,\n",
    "           'pred': pred,\n",
    "           'pred_softmax': pred_softmax,\n",
    "           'loss': loss}\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    fout_out_filelist = open(FLAGS.output_filelist, 'w')\n",
    "    for room_path in ROOM_PATH_LIST:\n",
    "        out_data_label_filename = os.path.basename(room_path)[:-4] + '_pred.txt'\n",
    "        out_data_label_filename = os.path.join(DUMP_DIR, out_data_label_filename)\n",
    "        out_gt_label_filename = os.path.basename(room_path)[:-4] + '_gt.txt'\n",
    "        out_gt_label_filename = os.path.join(DUMP_DIR, out_gt_label_filename)\n",
    "        print(room_path, out_data_label_filename)\n",
    "        a, b = eval_one_epoch(sess, ops, room_path, out_data_label_filename, out_gt_label_filename)\n",
    "        total_correct += a\n",
    "        total_seen += b\n",
    "        fout_out_filelist.write(out_data_label_filename+'\\n')\n",
    "    fout_out_filelist.close()\n",
    "    log_string('all room eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "\n",
    "def eval_one_epoch(sess, ops, room_path, out_data_label_filename, out_gt_label_filename):\n",
    "    error_cnt = 0\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    if FLAGS.visu:\n",
    "        fout = open(os.path.join(DUMP_DIR, os.path.basename(room_path)[:-4]+'_pred.obj'), 'w')\n",
    "        fout_gt = open(os.path.join(DUMP_DIR, os.path.basename(room_path)[:-4]+'_gt.obj'), 'w')\n",
    "    fout_data_label = open(out_data_label_filename, 'w')\n",
    "    fout_gt_label = open(out_gt_label_filename, 'w')\n",
    "    \n",
    "    current_data, current_label = indoor3d_util.room2blocks_wrapper_normalized(room_path, NUM_POINT)\n",
    "    current_data = current_data[:,0:NUM_POINT,:]\n",
    "    current_label = np.squeeze(current_label)\n",
    "    # Get room dimension..\n",
    "    data_label = np.load(room_path)\n",
    "    data = data_label[:,0:6]\n",
    "    max_room_x = max(data[:,0])\n",
    "    max_room_y = max(data[:,1])\n",
    "    max_room_z = max(data[:,2])\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    print(file_size)\n",
    "\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        cur_batch_size = end_idx - start_idx\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        loss_val, pred_val = sess.run([ops['loss'], ops['pred_softmax']],\n",
    "                                      feed_dict=feed_dict)\n",
    "\n",
    "        if FLAGS.no_clutter:\n",
    "            pred_label = np.argmax(pred_val[:,:,0:12], 2) # BxN\n",
    "        else:\n",
    "            pred_label = np.argmax(pred_val, 2) # BxN\n",
    "        # Save prediction labels to OBJ file\n",
    "        for b in range(BATCH_SIZE):\n",
    "            pts = current_data[start_idx+b, :, :]\n",
    "            l = current_label[start_idx+b,:]\n",
    "            pts[:,6] *= max_room_x\n",
    "            pts[:,7] *= max_room_y\n",
    "            pts[:,8] *= max_room_z\n",
    "            pts[:,3:6] *= 255.0\n",
    "            pred = pred_label[b, :]\n",
    "            for i in range(NUM_POINT):\n",
    "                color = indoor3d_util.g_label2color[pred[i]]\n",
    "                color_gt = indoor3d_util.g_label2color[current_label[start_idx+b, i]]\n",
    "                if FLAGS.visu:\n",
    "                    fout.write('v %f %f %f %d %d %d\\n' % (pts[i,6], pts[i,7], pts[i,8], color[0], color[1], color[2]))\n",
    "                    fout_gt.write('v %f %f %f %d %d %d\\n' % (pts[i,6], pts[i,7], pts[i,8], color_gt[0], color_gt[1], color_gt[2]))\n",
    "                fout_data_label.write('%f %f %f %d %d %d %f %d\\n' % (pts[i,6], pts[i,7], pts[i,8], pts[i,3], pts[i,4], pts[i,5], pred_val[b,i,pred[i]], pred[i]))\n",
    "                fout_gt_label.write('%d\\n' % (l[i]))\n",
    "        correct = np.sum(pred_label == current_label[start_idx:end_idx,:])\n",
    "        total_correct += correct\n",
    "        total_seen += (cur_batch_size*NUM_POINT)\n",
    "        loss_sum += (loss_val*BATCH_SIZE)\n",
    "        for i in range(start_idx, end_idx):\n",
    "            for j in range(NUM_POINT):\n",
    "                l = current_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_label[i-start_idx, j] == l)\n",
    "\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    fout_data_label.close()\n",
    "    fout_gt_label.close()\n",
    "    if FLAGS.visu:\n",
    "        fout.close()\n",
    "        fout_gt.close()\n",
    "    return total_correct, total_seen\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    with tf.Graph().as_default():\n",
    "        evaluate()\n",
    "    LOG_FOUT.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-draft",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "import tf_util\n",
    "\n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_point, 9))\n",
    "    labels_pl = tf.placeholder(tf.int32,\n",
    "                                shape=(batch_size, num_point))\n",
    "    return pointclouds_pl, labels_pl\n",
    "\n",
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" ConvNet baseline, input is BxNx3 gray image \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0].value\n",
    "    num_point = point_cloud.get_shape()[1].value\n",
    "\n",
    "    input_image = tf.expand_dims(point_cloud, -1)\n",
    "    # CONV\n",
    "    net = tf_util.conv2d(input_image, 64, [1,9], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv2', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 128, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv4', bn_decay=bn_decay)\n",
    "    points_feat1 = tf_util.conv2d(net, 1024, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv5', bn_decay=bn_decay)\n",
    "    # MAX\n",
    "    pc_feat1 = tf_util.max_pool2d(points_feat1, [num_point,1], padding='VALID', scope='maxpool1')\n",
    "    # FC\n",
    "    pc_feat1 = tf.reshape(pc_feat1, [batch_size, -1])\n",
    "    pc_feat1 = tf_util.fully_connected(pc_feat1, 256, bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)\n",
    "    pc_feat1 = tf_util.fully_connected(pc_feat1, 128, bn=True, is_training=is_training, scope='fc2', bn_decay=bn_decay)\n",
    "    print(pc_feat1)\n",
    "   \n",
    "    # CONCAT \n",
    "    pc_feat1_expand = tf.tile(tf.reshape(pc_feat1, [batch_size, 1, 1, -1]), [1, num_point, 1, 1])\n",
    "    points_feat1_concat = tf.concat(axis=3, values=[points_feat1, pc_feat1_expand])\n",
    "    \n",
    "    # CONV \n",
    "    net = tf_util.conv2d(points_feat1_concat, 512, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv6')\n",
    "    net = tf_util.conv2d(net, 256, [1,1], padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training, scope='conv7')\n",
    "    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training, scope='dp1')\n",
    "    net = tf_util.conv2d(net, 13, [1,1], padding='VALID', stride=[1,1],\n",
    "                         activation_fn=None, scope='conv8')\n",
    "    net = tf.squeeze(net, [2])\n",
    "\n",
    "    return net\n",
    "\n",
    "def get_loss(pred, label):\n",
    "    \"\"\" pred: B,N,13\n",
    "        label: B,N \"\"\"\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with tf.Graph().as_default():\n",
    "        a = tf.placeholder(tf.float32, shape=(32,4096,9))\n",
    "        net = get_model(a, tf.constant(True))\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            start = time.time()\n",
    "            for i in range(100):\n",
    "                print(i)\n",
    "                sess.run(net, feed_dict={a:np.random.rand(32,4096,9)})\n",
    "            print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-configuration",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(ROOT_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "import provider\n",
    "import tf_util\n",
    "from model import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')\n",
    "parser.add_argument('--log_dir', default='log', help='Log dir [default: log]')\n",
    "parser.add_argument('--num_point', type=int, default=4096, help='Point number [default: 4096]')\n",
    "parser.add_argument('--max_epoch', type=int, default=50, help='Epoch to run [default: 50]')\n",
    "parser.add_argument('--batch_size', type=int, default=24, help='Batch Size during training [default: 24]')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='Initial learning rate [default: 0.9]')\n",
    "parser.add_argument('--optimizer', default='adam', help='adam or momentum [default: adam]')\n",
    "parser.add_argument('--decay_step', type=int, default=300000, help='Decay step for lr decay [default: 300000]')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.5, help='Decay rate for lr decay [default: 0.5]')\n",
    "parser.add_argument('--test_area', type=int, default=6, help='Which area to use for test, option: 1-6 [default: 6]')\n",
    "FLAGS = parser.parse_args()\n",
    "\n",
    "\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "NUM_POINT = FLAGS.num_point\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate\n",
    "\n",
    "LOG_DIR = FLAGS.log_dir\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "MAX_NUM_POINT = 4096\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "ALL_FILES = provider.getDataFiles('indoor3d_sem_seg_hdf5_data/all_files.txt')\n",
    "room_filelist = [line.rstrip() for line in open('indoor3d_sem_seg_hdf5_data/room_filelist.txt')]\n",
    "\n",
    "# Load ALL data\n",
    "data_batch_list = []\n",
    "label_batch_list = []\n",
    "for h5_filename in ALL_FILES:\n",
    "    data_batch, label_batch = provider.loadDataFile(h5_filename)\n",
    "    data_batch_list.append(data_batch)\n",
    "    label_batch_list.append(label_batch)\n",
    "data_batches = np.concatenate(data_batch_list, 0)\n",
    "label_batches = np.concatenate(label_batch_list, 0)\n",
    "print(data_batches.shape)\n",
    "print(label_batches.shape)\n",
    "\n",
    "test_area = 'Area_'+str(FLAGS.test_area)\n",
    "train_idxs = []\n",
    "test_idxs = []\n",
    "for i,room_name in enumerate(room_filelist):\n",
    "    if test_area in room_name:\n",
    "        test_idxs.append(i)\n",
    "    else:\n",
    "        train_idxs.append(i)\n",
    "\n",
    "train_data = data_batches[train_idxs,...]\n",
    "train_label = label_batches[train_idxs]\n",
    "test_data = data_batches[test_idxs,...]\n",
    "test_label = label_batches[test_idxs]\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = get_loss(pred, labels_pl)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(pred, 2), tf.to_int64(labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                                  sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init, {is_training_pl:True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            log_string('**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "             \n",
    "            train_one_epoch(sess, ops, train_writer)\n",
    "            eval_one_epoch(sess, ops, test_writer)\n",
    "            \n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string('----')\n",
    "    current_data, current_label, _ = provider.shuffle_data(train_data[:,0:NUM_POINT,:], train_label) \n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "    \n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    current_data = test_data[:,0:NUM_POINT,:]\n",
    "    current_label = np.squeeze(test_label)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        test_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += (loss_val*BATCH_SIZE)\n",
    "        for i in range(start_idx, end_idx):\n",
    "            for j in range(NUM_POINT):\n",
    "                l = current_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx, j] == l)\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    LOG_FOUT.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-think",
   "metadata": {},
   "source": [
    "### Evaluation and Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_data_label_filenames = [line.rstrip() for line in open('all_pred_data_label_filelist.txt')]\n",
    "gt_label_filenames = [f.rstrip('_pred\\.txt') + '_gt.txt' for f in pred_data_label_filenames]\n",
    "num_room = len(gt_label_filenames)\n",
    "\n",
    "\n",
    "gt_classes = [0 for _ in range(13)]\n",
    "positive_classes = [0 for _ in range(13)]\n",
    "true_positive_classes = [0 for _ in range(13)]\n",
    "for i in range(num_room):\n",
    "    print(i)\n",
    "    data_label = np.loadtxt(pred_data_label_filenames[i])\n",
    "    pred_label = data_label[:,-1]\n",
    "    gt_label = np.loadtxt(gt_label_filenames[i])\n",
    "    print(gt_label.shape)\n",
    "    for j in xrange(gt_label.shape[0]):\n",
    "        gt_l = int(gt_label[j])\n",
    "        pred_l = int(pred_label[j])\n",
    "        gt_classes[gt_l] += 1\n",
    "        positive_classes[pred_l] += 1\n",
    "        true_positive_classes[gt_l] += int(gt_l==pred_l)\n",
    "\n",
    "\n",
    "print(gt_classes)\n",
    "print(positive_classes)\n",
    "print(true_positive_classes)\n",
    "\n",
    "\n",
    "print('Overall accuracy: {0}'.format(sum(true_positive_classes)/float(sum(positive_classes))))\n",
    "\n",
    "print 'IoU:'\n",
    "iou_list = []\n",
    "for i in range(13):\n",
    "    iou = true_positive_classes[i]/float(gt_classes[i]+positive_classes[i]-true_positive_classes[i]) \n",
    "    print(iou)\n",
    "    iou_list.append(iou)\n",
    "\n",
    "print(sum(iou_list)/13.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
